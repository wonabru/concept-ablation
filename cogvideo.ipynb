{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fabd25df-3d6e-41f7-9623-3098f062ee7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/usr/local/lib/python3.11/dist-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e01f298b0d34544be1e3aa734daa911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "549ea2879d6441e5b2e67900daa37af6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95d25e1551504a7f88839768a88ce511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "Found an existing imageio backend in your environment. Attempting to export video with imageio. \nUnable to find a compatible ffmpeg installation in your environment to use with imageio. Please install via `pip install imageio-ffmpeg",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/utils/export_utils.py:162\u001b[0m, in \u001b[0;36mexport_to_video\u001b[0;34m(video_frames, output_video_path, fps)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 162\u001b[0m     \u001b[43mimageio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplugins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mffmpeg\u001b[49m\u001b[38;5;241m.\u001b[39mget_exe()\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/imageio/plugins/__init__.py:103\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'imageio.plugins' has no attribute 'ffmpeg'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 23\u001b[0m\n\u001b[1;32m     12\u001b[0m pipe\u001b[38;5;241m.\u001b[39mvae\u001b[38;5;241m.\u001b[39menable_tiling()\n\u001b[1;32m     14\u001b[0m video \u001b[38;5;241m=\u001b[39m pipe(\n\u001b[1;32m     15\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m     16\u001b[0m     num_videos_per_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     generator\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mGenerator(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m42\u001b[39m),\n\u001b[1;32m     21\u001b[0m )\u001b[38;5;241m.\u001b[39mframes[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 23\u001b[0m \u001b[43mexport_to_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput.mp4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/utils/export_utils.py:164\u001b[0m, in \u001b[0;36mexport_to_video\u001b[0;34m(video_frames, output_video_path, fps)\u001b[0m\n\u001b[1;32m    162\u001b[0m     imageio\u001b[38;5;241m.\u001b[39mplugins\u001b[38;5;241m.\u001b[39mffmpeg\u001b[38;5;241m.\u001b[39mget_exe()\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    165\u001b[0m         (\n\u001b[1;32m    166\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound an existing imageio backend in your environment. Attempting to export video with imageio. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    167\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to find a compatible ffmpeg installation in your environment to use with imageio. Please install via `pip install imageio-ffmpeg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m         )\n\u001b[1;32m    169\u001b[0m     )\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_video_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    172\u001b[0m     output_video_path \u001b[38;5;241m=\u001b[39m tempfile\u001b[38;5;241m.\u001b[39mNamedTemporaryFile(suffix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mname\n",
      "\u001b[0;31mAttributeError\u001b[0m: Found an existing imageio backend in your environment. Attempting to export video with imageio. \nUnable to find a compatible ffmpeg installation in your environment to use with imageio. Please install via `pip install imageio-ffmpeg"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import CogVideoXPipeline\n",
    "from diffusers.utils import export_to_video\n",
    "\n",
    "prompt = \"A panda, dressed in a small, red jacket and a tiny hat, sits on a wooden stool in a serene bamboo forest. The panda's fluffy paws strum a miniature acoustic guitar, producing soft, melodic tunes. Nearby, a few other pandas gather, watching curiously and some clapping in rhythm. Sunlight filters through the tall bamboo, casting a gentle glow on the scene. The panda's face is expressive, showing concentration and joy as it plays. The background includes a small, flowing stream and vibrant green foliage, enhancing the peaceful and magical atmosphere of this unique musical performance.\"\n",
    "pipe = CogVideoXPipeline.from_pretrained(\n",
    "    \"THUDM/CogVideoX-5b\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "pipe.enable_model_cpu_offload()\n",
    "pipe.vae.enable_tiling()\n",
    "\n",
    "video = pipe(\n",
    "    prompt=prompt,\n",
    "    num_videos_per_prompt=1,\n",
    "    num_inference_steps=50,\n",
    "    num_frames=49,\n",
    "    guidance_scale=6,\n",
    "    generator=torch.Generator(device=\"cuda\").manual_seed(42),\n",
    ").frames[0]\n",
    "\n",
    "export_to_video(video, \"output.mp4\", fps=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4e03724-b05a-465a-aa11-9eb6259a4ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40aeb44f0c204d32a3159b56fbbdb55e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/3.74G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e40be2bbd5e47d090a933e96ee6fe49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/4289 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# Replace 'your_dataset_name' with the actual dataset name\n",
    "dataset = load_dataset(\"yesidobyte/nsfw1024\", split='train')\n",
    "# Specify the directory to save images\n",
    "save_dir = '/workspace/data/samples_nudity/images'\n",
    "import os\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "# Select the first 200 images\n",
    "first_200_images = dataset.select(range(200))\n",
    "# Access the images\n",
    "for i, example in enumerate(first_200_images):\n",
    "    # Replace 'image_column' with the actual column name containing image URLs or data\n",
    "    image = example['image']\n",
    "    try:\n",
    "        # If the image is a URL, download it\n",
    "        if isinstance(image, str):\n",
    "            response = requests.get(image)\n",
    "            with open(f\"{save_dir}/image_{i}.jpg\", 'wb') as f:\n",
    "                f.write(response.content)\n",
    "        # If the image is a PIL Image object\n",
    "        elif isinstance(image, Image.Image):\n",
    "            with open(f\"{save_dir}/image_{i}.jpg\", 'wb') as f:\n",
    "                image.save(f, format='JPEG')\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16155da1-5a86-4af3-a898-c358e56c5122",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/workspace/data/samples_nudity/caption.txt', 'w') as file:\n",
    "    for _ in range(200):\n",
    "        file.write('naked woman\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f751b05e-370f-4b35-b975-d247682d2935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-09-05 02:45:47,512] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "W0905 02:45:48.648000 139961514689664 torch/distributed/run.py:779] \n",
      "W0905 02:45:48.648000 139961514689664 torch/distributed/run.py:779] *****************************************\n",
      "W0905 02:45:48.648000 139961514689664 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0905 02:45:48.648000 139961514689664 torch/distributed/run.py:779] *****************************************\n",
      "/usr/local/lib/python3.11/dist-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/usr/local/lib/python3.11/dist-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n",
      "/usr/local/lib/python3.11/dist-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/usr/local/lib/python3.11/dist-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n",
      "/workspace/concept-ablation/diffusers/train.py:541: UserWarning: You need not use --class_data_dir without --with_prior_preservation.\n",
      "  warnings.warn(\n",
      "/workspace/concept-ablation/diffusers/train.py:545: UserWarning: You need not use --class_prompt without --with_prior_preservation.\n",
      "  warnings.warn(\n",
      "[2024-09-05 02:45:53,993] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "/workspace/concept-ablation/diffusers/train.py:541: UserWarning: You need not use --class_data_dir without --with_prior_preservation.\n",
      "  warnings.warn(\n",
      "/workspace/concept-ablation/diffusers/train.py:545: UserWarning: You need not use --class_prompt without --with_prior_preservation.\n",
      "  warnings.warn(\n",
      "[2024-09-05 02:45:54,078] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-09-05 02:45:55,098] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2024-09-05 02:45:55,246] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2024-09-05 02:45:55,246] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "The current device has compute capability of (8, 6) which is insufficient for FP8 mixed precision training (requires a GPU Hopper/Ada Lovelace or higher, compute capability of 8.9 or higher). Will use FP16 instead.\n",
      "/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py:412: UserWarning: `log_with=tensorboard` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n",
      "The current device has compute capability of (8, 6) which is insufficient for FP8 mixed precision training (requires a GPU Hopper/Ada Lovelace or higher, compute capability of 8.9 or higher). Will use FP16 instead.\n",
      "09/05/2024 02:45:55 - INFO - __main__ - Distributed environment: DistributedType.DEEPSPEED  Backend: nccl\n",
      "Num processes: 2\n",
      "Process index: 1\n",
      "Local process index: 1\n",
      "Device: cuda:1\n",
      "\n",
      "Mixed precision type: bf16\n",
      "ds_config: {'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'gradient_accumulation_steps': 1, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'none', 'nvme_path': None}, 'offload_param': {'device': 'none', 'nvme_path': None}, 'stage3_gather_16bit_weights_on_model_save': False}, 'gradient_clipping': 'auto', 'steps_per_print': inf, 'bf16': {'enabled': True}, 'fp16': {'enabled': False}}\n",
      "\n",
      "/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py:412: UserWarning: `log_with=tensorboard` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "09/05/2024 02:45:55 - INFO - __main__ - Distributed environment: DistributedType.DEEPSPEED  Backend: nccl\n",
      "Num processes: 2\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda:0\n",
      "\n",
      "Mixed precision type: bf16\n",
      "ds_config: {'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'gradient_accumulation_steps': 1, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'none', 'nvme_path': None}, 'offload_param': {'device': 'none', 'nvme_path': None}, 'stage3_gather_16bit_weights_on_model_save': False}, 'gradient_clipping': 'auto', 'steps_per_print': inf, 'bf16': {'enabled': True}, 'fp16': {'enabled': False}}\n",
      "\n",
      "{'pretrained_model_name_or_path': 'THUDM/CogVideoX-5b', 'revision': None, 'tokenizer_name': None, 'concept_type': 'nudity', 'caption_target': 'nudity, nsfw', 'instance_data_dir': None, 'class_data_dir': '/workspace/data/samples_nudity', 'instance_prompt': None, 'class_prompt': 'people, body', 'mem_impath': '', 'validation_prompt': None, 'num_validation_images': 2, 'validation_steps': 500, 'with_prior_preservation': False, 'prior_loss_weight': 1.0, 'train_size': 100, 'output_dir': '/workspace/log_ouput', 'num_class_images': 1000, 'num_class_prompts': 200, 'seed': 42, 'resolution': 512, 'center_crop': False, 'train_batch_size': 2, 'sample_batch_size': 4, 'num_train_epochs': 1, 'max_train_steps': 4, 'checkpointing_steps': 250, 'checkpoints_total_limit': None, 'resume_from_checkpoint': None, 'gradient_accumulation_steps': 1, 'gradient_checkpointing': False, 'learning_rate': 2e-07, 'scale_lr': True, 'dataloader_num_workers': 2, 'parameter_group': 'full-weight', 'loss_type_reverse': 'model-based', 'lr_scheduler': 'constant', 'lr_warmup_steps': 10, 'use_8bit_adam': True, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_weight_decay': 0.01, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'push_to_hub': False, 'hub_token': None, 'hub_model_id': None, 'logging_dir': 'logs', 'allow_tf32': False, 'report_to': 'tensorboard', 'mixed_precision': None, 'prior_generation_precision': None, 'concepts_list': None, 'local_rank': 0, 'enable_xformers_memory_efficient_attention': True, 'hflip': True, 'noaug': False}\n",
      "Keyword arguments {'safety_checker': None} are not expected by CogVideoXPipeline and will be ignored.\n",
      "Loading pipeline components...:   0%|                     | 0/5 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as T5Tokenizer from `tokenizer` subfolder of THUDM/CogVideoX-5b.\n",
      "Loading pipeline components...:  20%|██▌          | 1/5 [00:00<00:00,  5.43it/s]\n",
      "Loading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Loading checkpoint shards:  50%|█████████         | 1/2 [00:03<00:03,  3.95s/it]\u001b[A\n",
      "Loading checkpoint shards:  50%|█████████         | 1/2 [00:04<00:04,  4.26s/it]\u001b[A\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:07<00:00,  3.66s/it]\u001b[A\n",
      "Loading pipeline components...:  20%|██▌          | 1/5 [00:07<00:29,  7.40s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:07<00:00,  3.94s/it]\u001b[A\n",
      "Loaded text_encoder as T5EncoderModel from `text_encoder` subfolder of THUDM/CogVideoX-5b.\n",
      "Loading pipeline components...:  40%|█████▏       | 2/5 [00:08<00:14,  4.75s/it]Loaded scheduler as CogVideoXDDIMScheduler from `scheduler` subfolder of THUDM/CogVideoX-5b.\n",
      "Loaded vae as AutoencoderKLCogVideoX from `vae` subfolder of THUDM/CogVideoX-5b.\n",
      "Loading pipeline components...: 100%|█████████████| 5/5 [00:16<00:00,  3.31s/it]\n",
      "Loaded transformer as CogVideoXTransformer3DModel from `transformer` subfolder of THUDM/CogVideoX-5b.\n",
      "Loading pipeline components...: 100%|█████████████| 5/5 [00:17<00:00,  3.53s/it]\n",
      "{'lower_order_final', 'solver_order', 'dynamic_thresholding_ratio', 'algorithm_type', 'euler_at_final', 'variance_type', 'solver_type', 'lambda_min_clipped', 'final_sigmas_type', 'thresholding', 'use_karras_sigmas', 'use_lu_lambdas'} was not found in config. Values will be initialized to default values.\n",
      "09/05/2024 02:46:19 - INFO - __main__ - Number of class images to sample: 10.\n",
      "Generating class images:   0%|                            | 0/2 [14:12<?, ?it/s]\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/workspace/concept-ablation/diffusers/train.py\", line 1363, in <module>\n",
      "[rank0]:     main(args)\n",
      "[rank0]:   File \"/workspace/concept-ablation/diffusers/train.py\", line 746, in main\n",
      "[rank0]:     images = pipeline(\n",
      "[rank0]:              ^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "[rank0]:     return func(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/diffusers/pipelines/cogvideo/pipeline_cogvideox.py\", line 735, in __call__\n",
      "[rank0]:     video = self.decode_latents(latents)\n",
      "[rank0]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/diffusers/pipelines/cogvideo/pipeline_cogvideox.py\", line 360, in decode_latents\n",
      "[rank0]:     frames = self.vae.decode(latents).sample\n",
      "[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/diffusers/utils/accelerate_utils.py\", line 46, in wrapper\n",
      "[rank0]:     return method(self, *args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/diffusers/models/autoencoders/autoencoder_kl_cogvideox.py\", line 1153, in decode\n",
      "[rank0]:     decoded = self._decode(z).sample\n",
      "[rank0]:               ^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/diffusers/models/autoencoders/autoencoder_kl_cogvideox.py\", line 1123, in _decode\n",
      "[rank0]:     z_intermediate = self.decoder(z_intermediate)\n",
      "[rank0]:                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "[rank0]:     return self._call_impl(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "[rank0]:     return forward_call(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/diffusers/models/autoencoders/autoencoder_kl_cogvideox.py\", line 877, in forward\n",
      "[rank0]:     hidden_states = up_block(hidden_states, temb, sample)\n",
      "[rank0]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "[rank0]:     return self._call_impl(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "[rank0]:     return forward_call(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/diffusers/models/autoencoders/autoencoder_kl_cogvideox.py\", line 606, in forward\n",
      "[rank0]:     hidden_states = upsampler(hidden_states)\n",
      "[rank0]:                     ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "[rank0]:     return self._call_impl(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "[rank0]:     return forward_call(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/diffusers/models/upsampling.py\", line 409, in forward\n",
      "[rank0]:     inputs = self.conv(inputs)\n",
      "[rank0]:              ^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "[rank0]:     return self._call_impl(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "[rank0]:     return forward_call(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 458, in forward\n",
      "[rank0]:     return self._conv_forward(input, self.weight, self.bias)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 454, in _conv_forward\n",
      "[rank0]:     return F.conv2d(input, weight, bias, self.stride,\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 5.93 GiB. GPU 0 has a total capacity of 44.35 GiB of which 1.11 GiB is free. Process 4122859 has 43.23 GiB memory in use. Of the allocated memory 36.30 GiB is allocated by PyTorch, and 6.43 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "W0905 03:00:32.793000 139961514689664 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3417 closing signal SIGTERM\n",
      "E0905 03:00:34.110000 139961514689664 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 3416) of binary: /usr/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/accelerate\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/accelerate/commands/accelerate_cli.py\", line 48, in main\n",
      "    args.func(args)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/accelerate/commands/launch.py\", line 1159, in launch_command\n",
      "    deepspeed_launcher(args)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/accelerate/commands/launch.py\", line 852, in deepspeed_launcher\n",
      "    distrib_run.run(args)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/run.py\", line 892, in run\n",
      "    elastic_launch(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py\", line 133, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py\", line 264, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "train.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2024-09-05_03:00:32\n",
      "  host      : a7204fa5dac5\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 3416)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!cd /workspace/concept-ablation/diffusers  && accelerate launch train.py \\\n",
    "          --pretrained_model_name_or_path=\"THUDM/CogVideoX-5b\"  \\\n",
    "          --output_dir=\"/workspace/log_ouput\" \\\n",
    "          --class_data_dir=\"/workspace/data/samples_nudity\" \\\n",
    "          --class_prompt=\"people, body\"  \\\n",
    "          --caption_target \"nudity, nsfw\" \\\n",
    "          --concept_type nudity \\\n",
    "          --resolution=512  \\\n",
    "          --train_batch_size=2  \\\n",
    "          --learning_rate=2e-7  \\\n",
    "          --max_train_steps=4 \\\n",
    "          --scale_lr --hflip --noaug \\\n",
    "          --train_size 100 \\\n",
    "          --lr_warmup_steps 10 \\\n",
    "          --use_8bit_adam \\\n",
    "          --parameter_group full-weight \\\n",
    "          --enable_xformers_memory_efficient_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bf1061-41cb-4d8f-b0fe-1590a7a51ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
