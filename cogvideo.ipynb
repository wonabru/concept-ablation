{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fabd25df-3d6e-41f7-9623-3098f062ee7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/usr/local/lib/python3.11/dist-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea21c1c7c284b34872e6458be75dd07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93e60171500140349d4d860afbf9260e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd263b7d35574657b8f5aa73d3468fea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'output.mp4'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import CogVideoXPipeline\n",
    "from diffusers.utils import export_to_video\n",
    "\n",
    "prompt = \"A panda, dressed in a small, red jacket and a tiny hat, sits on a wooden stool in a serene bamboo forest. The panda's fluffy paws strum a miniature acoustic guitar, producing soft, melodic tunes. Nearby, a few other pandas gather, watching curiously and some clapping in rhythm. Sunlight filters through the tall bamboo, casting a gentle glow on the scene. The panda's face is expressive, showing concentration and joy as it plays. The background includes a small, flowing stream and vibrant green foliage, enhancing the peaceful and magical atmosphere of this unique musical performance.\"\n",
    "pipe = CogVideoXPipeline.from_pretrained(\n",
    "    \"THUDM/CogVideoX-5b\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "pipe.enable_model_cpu_offload()\n",
    "pipe.vae.enable_tiling()\n",
    "\n",
    "video = pipe(\n",
    "    prompt=prompt,\n",
    "    num_videos_per_prompt=1,\n",
    "    num_inference_steps=50,\n",
    "    num_frames=49,\n",
    "    guidance_scale=6,\n",
    "    generator=torch.Generator(device=\"cuda\").manual_seed(42),\n",
    ").frames[0]\n",
    "\n",
    "export_to_video(video, \"output.mp4\", fps=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4e03724-b05a-465a-aa11-9eb6259a4ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb162d6a0241492085c5f42c130793b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/3.74G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a62483a5854b48d58fdda724b705e604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/4289 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# Replace 'your_dataset_name' with the actual dataset name\n",
    "dataset = load_dataset(\"yesidobyte/nsfw1024\", split='train')\n",
    "# Specify the directory to save images\n",
    "save_dir = '/workspace/data/samples_nudity/images'\n",
    "import os\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "# Select the first 200 images\n",
    "first_200_images = dataset.select(range(200))\n",
    "# Access the images\n",
    "for i, example in enumerate(first_200_images):\n",
    "    # Replace 'image_column' with the actual column name containing image URLs or data\n",
    "    image = example['image']\n",
    "    try:\n",
    "        # If the image is a URL, download it\n",
    "        if isinstance(image, str):\n",
    "            response = requests.get(image)\n",
    "            with open(f\"{save_dir}/image_{i}.jpg\", 'wb') as f:\n",
    "                f.write(response.content)\n",
    "        # If the image is a PIL Image object\n",
    "        elif isinstance(image, Image.Image):\n",
    "            with open(f\"{save_dir}/image_{i}.jpg\", 'wb') as f:\n",
    "                image.save(f, format='JPEG')\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16155da1-5a86-4af3-a898-c358e56c5122",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/workspace/data/samples_nudity/caption.txt', 'w') as file:\n",
    "    for _ in range(200):\n",
    "        file.write('naked woman\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f751b05e-370f-4b35-b975-d247682d2935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-09-05 05:42:58,249] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "df: /root/.triton/autotune: No such file or directory\n",
      "W0905 05:42:59.175000 131125833850880 torch/distributed/run.py:779] \n",
      "W0905 05:42:59.175000 131125833850880 torch/distributed/run.py:779] *****************************************\n",
      "W0905 05:42:59.175000 131125833850880 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0905 05:42:59.175000 131125833850880 torch/distributed/run.py:779] *****************************************\n",
      "/usr/local/lib/python3.11/dist-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/usr/local/lib/python3.11/dist-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/usr/local/lib/python3.11/dist-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n",
      "/usr/local/lib/python3.11/dist-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n",
      "/workspace/concept-ablation/diffusers/train.py:541: UserWarning: You need not use --class_data_dir without --with_prior_preservation.\n",
      "  warnings.warn(\n",
      "/workspace/concept-ablation/diffusers/train.py:545: UserWarning: You need not use --class_prompt without --with_prior_preservation.\n",
      "  warnings.warn(\n",
      "[2024-09-05 05:43:04,398] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "/workspace/concept-ablation/diffusers/train.py:541: UserWarning: You need not use --class_data_dir without --with_prior_preservation.\n",
      "  warnings.warn(\n",
      "/workspace/concept-ablation/diffusers/train.py:545: UserWarning: You need not use --class_prompt without --with_prior_preservation.\n",
      "  warnings.warn(\n",
      "[2024-09-05 05:43:04,506] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-09-05 05:43:05,322] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2024-09-05 05:43:05,549] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2024-09-05 05:43:05,549] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "The current device has compute capability of (8, 0) which is insufficient for FP8 mixed precision training (requires a GPU Hopper/Ada Lovelace or higher, compute capability of 8.9 or higher). Will use FP16 instead.\n",
      "/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py:412: UserWarning: `log_with=tensorboard` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n",
      "09/05/2024 05:43:05 - INFO - __main__ - Distributed environment: DistributedType.DEEPSPEED  Backend: nccl\n",
      "Num processes: 2\n",
      "Process index: 1\n",
      "Local process index: 1\n",
      "Device: cuda:1\n",
      "\n",
      "Mixed precision type: bf16\n",
      "ds_config: {'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'gradient_accumulation_steps': 1, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'none', 'nvme_path': None}, 'offload_param': {'device': 'none', 'nvme_path': None}, 'stage3_gather_16bit_weights_on_model_save': False}, 'gradient_clipping': 'auto', 'steps_per_print': inf, 'bf16': {'enabled': True}, 'fp16': {'enabled': False}}\n",
      "\n",
      "The current device has compute capability of (8, 0) which is insufficient for FP8 mixed precision training (requires a GPU Hopper/Ada Lovelace or higher, compute capability of 8.9 or higher). Will use FP16 instead.\n",
      "/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py:412: UserWarning: `log_with=tensorboard` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n",
      "09/05/2024 05:43:05 - INFO - __main__ - Distributed environment: DistributedType.DEEPSPEED  Backend: nccl\n",
      "Num processes: 2\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda:0\n",
      "\n",
      "Mixed precision type: bf16\n",
      "ds_config: {'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'gradient_accumulation_steps': 1, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'none', 'nvme_path': None}, 'offload_param': {'device': 'none', 'nvme_path': None}, 'stage3_gather_16bit_weights_on_model_save': False}, 'gradient_clipping': 'auto', 'steps_per_print': inf, 'bf16': {'enabled': True}, 'fp16': {'enabled': False}}\n",
      "\n",
      "{'pretrained_model_name_or_path': 'THUDM/CogVideoX-5b', 'revision': None, 'tokenizer_name': None, 'concept_type': 'nudity', 'caption_target': 'nudity, nsfw', 'instance_data_dir': None, 'class_data_dir': '/workspace/data/samples_nudity', 'instance_prompt': None, 'class_prompt': 'people, body', 'mem_impath': '', 'validation_prompt': None, 'num_validation_images': 2, 'validation_steps': 500, 'with_prior_preservation': False, 'prior_loss_weight': 1.0, 'train_size': 100, 'output_dir': '/workspace/log_ouput', 'num_class_images': 1000, 'num_class_prompts': 200, 'seed': 42, 'resolution': 512, 'center_crop': False, 'train_batch_size': 2, 'sample_batch_size': 4, 'num_train_epochs': 1, 'max_train_steps': 4, 'checkpointing_steps': 250, 'checkpoints_total_limit': None, 'resume_from_checkpoint': None, 'gradient_accumulation_steps': 1, 'gradient_checkpointing': False, 'learning_rate': 2e-07, 'scale_lr': True, 'dataloader_num_workers': 2, 'parameter_group': 'full-weight', 'loss_type_reverse': 'model-based', 'lr_scheduler': 'constant', 'lr_warmup_steps': 10, 'use_8bit_adam': True, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_weight_decay': 0.01, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'push_to_hub': False, 'hub_token': None, 'hub_model_id': None, 'logging_dir': 'logs', 'allow_tf32': False, 'report_to': 'tensorboard', 'mixed_precision': None, 'prior_generation_precision': None, 'concepts_list': None, 'local_rank': 0, 'enable_xformers_memory_efficient_attention': True, 'hflip': True, 'noaug': True}\n",
      "Loading pipeline components...:   0%|                     | 0/5 [00:00<?, ?it/s]Keyword arguments {'safety_checker': None} are not expected by CogVideoXPipeline and will be ignored.\n",
      "Loading pipeline components...:   0%|                     | 0/5 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Loading checkpoint shards:  50%|█████████         | 1/2 [00:04<00:04,  4.98s/it]\u001b[A\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:09<00:00,  4.88s/it]\u001b[A\n",
      "Loading pipeline components...:  80%|██████████▍  | 4/5 [00:10<00:02,  2.03s/it]Loaded transformer as CogVideoXTransformer3DModel from `transformer` subfolder of THUDM/CogVideoX-5b.\n",
      "Loading pipeline components...:  20%|██▌          | 1/5 [00:12<00:51, 12.90s/it]Loaded scheduler as CogVideoXDDIMScheduler from `scheduler` subfolder of THUDM/CogVideoX-5b.\n",
      "\n",
      "Loading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Loading checkpoint shards:  50%|█████████         | 1/2 [00:05<00:05,  5.05s/it]\u001b[A\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:09<00:00,  4.83s/it]\u001b[A\n",
      "Loaded text_encoder as T5EncoderModel from `text_encoder` subfolder of THUDM/CogVideoX-5b.\n",
      "Loading pipeline components...: 100%|█████████████| 5/5 [00:22<00:00,  4.59s/it]\n",
      "Loaded vae as AutoencoderKLCogVideoX from `vae` subfolder of THUDM/CogVideoX-5b.\n",
      "Loading pipeline components...:  80%|██████████▍  | 4/5 [00:23<00:04,  4.71s/it]Loaded tokenizer as T5Tokenizer from `tokenizer` subfolder of THUDM/CogVideoX-5b.\n",
      "Loading pipeline components...: 100%|█████████████| 5/5 [00:23<00:00,  4.65s/it]\n",
      "{'use_lu_lambdas', 'variance_type', 'euler_at_final', 'lower_order_final', 'final_sigmas_type', 'use_karras_sigmas', 'thresholding', 'dynamic_thresholding_ratio', 'solver_type', 'algorithm_type', 'solver_order', 'lambda_min_clipped'} was not found in config. Values will be initialized to default values.\n",
      "09/05/2024 05:43:33 - INFO - __main__ - Number of class images to sample: 10.\n",
      "Generating class images:   0%|                            | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "!cd /workspace/concept-ablation/diffusers  && accelerate launch train.py \\\n",
    "          --pretrained_model_name_or_path=\"THUDM/CogVideoX-5b\"  \\\n",
    "          --output_dir=\"/workspace/log_ouput\" \\\n",
    "          --class_data_dir=\"/workspace/data/samples_nudity\" \\\n",
    "          --class_prompt=\"people, body\"  \\\n",
    "          --caption_target \"nudity, nsfw\" \\\n",
    "          --concept_type nudity \\\n",
    "          --resolution=512  \\\n",
    "          --train_batch_size=2  \\\n",
    "          --learning_rate=2e-7  \\\n",
    "          --max_train_steps=4 \\\n",
    "          --scale_lr --hflip --noaug \\\n",
    "          --train_size 100 \\\n",
    "          --lr_warmup_steps 10 \\\n",
    "          --use_8bit_adam \\\n",
    "          --parameter_group full-weight \\\n",
    "          --enable_xformers_memory_efficient_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bf1061-41cb-4d8f-b0fe-1590a7a51ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
