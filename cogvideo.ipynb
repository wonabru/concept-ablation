{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fabd25df-3d6e-41f7-9623-3098f062ee7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/usr/local/lib/python3.11/dist-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae3ba7b88b7f4d6eb586cac36daf0856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_index.json:   0%|          | 0.00/411 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c6904213aa14755ae2674deec2d2b30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 16 files:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03b278adeba945739380d0e4d37b231a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61a291bc2ae14dc0a326a0b4d98ea49c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.53G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1104dadebaca409c926d92688ee0a193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5b7bd1c61d242eab619e027ca9645ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler/scheduler_config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adaa1a9b93214a73ae318e74c5f25ed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "text_encoder/config.json:   0%|          | 0.00/809 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab486f7960134606ad0f5751ba976c6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer/added_tokens.json:   0%|          | 0.00/2.59k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d66868acbbf346d6a916332ab9b97e10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)ext_encoder/model.safetensors.index.json:   0%|          | 0.00/19.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d344602c9aa5425aad873a979559fd4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer/special_tokens_map.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2cb8f69a9b0440ebd5cef4a528eee0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer/tokenizer_config.json:   0%|          | 0.00/20.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06b8545b65114b618520e6c4c20bd1f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)pytorch_model-00001-of-00002.safetensors:   0%|          | 0.00/9.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53c44c32b2f24db685fc3a72ed1cb8c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)pytorch_model-00002-of-00002.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d7c6e42bae449a880d0cb56314c08f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "transformer/config.json:   0%|          | 0.00/757 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b386059ed8e84a9fb59e4fd98784995d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)ion_pytorch_model.safetensors.index.json:   0%|          | 0.00/103k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaf42b71249d4215a440376842ec462b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vae/config.json:   0%|          | 0.00/839 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f08f9d77747488aae0434441dc6a14b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "diffusion_pytorch_model.safetensors:   0%|          | 0.00/431M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf25a0a663e8448e9ba12def0028e8bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d38c6e3672d4ec8809665b67b6ccaac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16545fb5bb844694b5d25878db6a7269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m pipe\u001b[38;5;241m.\u001b[39menable_model_cpu_offload()\n\u001b[1;32m     12\u001b[0m pipe\u001b[38;5;241m.\u001b[39mvae\u001b[38;5;241m.\u001b[39menable_tiling()\n\u001b[0;32m---> 14\u001b[0m video \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_videos_per_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m49\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mframes[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     23\u001b[0m export_to_video(video, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput.mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m, fps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/pipelines/cogvideo/pipeline_cogvideox.py:707\u001b[0m, in \u001b[0;36mCogVideoXPipeline.__call__\u001b[0;34m(self, prompt, negative_prompt, height, width, num_frames, num_inference_steps, timesteps, guidance_scale, use_dynamic_cfg, num_videos_per_prompt, eta, generator, latents, prompt_embeds, negative_prompt_embeds, output_type, return_dict, callback_on_step_end, callback_on_step_end_tensor_inputs, max_sequence_length)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# compute the previous noisy sample x_t -> x_t-1\u001b[39;00m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler, CogVideoXDPMScheduler):\n\u001b[0;32m--> 707\u001b[0m     latents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoise_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_step_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    709\u001b[0m     latents, old_pred_original_sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mstep(\n\u001b[1;32m    710\u001b[0m         noise_pred,\n\u001b[1;32m    711\u001b[0m         old_pred_original_sample,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    716\u001b[0m         return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    717\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/schedulers/scheduling_ddim_cogvideox.py:371\u001b[0m, in \u001b[0;36mCogVideoXDDIMScheduler.step\u001b[0;34m(self, model_output, timestep, sample, eta, use_clipped_model_output, generator, variance_noise, return_dict)\u001b[0m\n\u001b[1;32m    368\u001b[0m alpha_prod_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malphas_cumprod[timestep]\n\u001b[1;32m    369\u001b[0m alpha_prod_t_prev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malphas_cumprod[prev_timestep] \u001b[38;5;28;01mif\u001b[39;00m prev_timestep \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_alpha_cumprod\n\u001b[0;32m--> 371\u001b[0m beta_prod_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43malpha_prod_t\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;66;03m# 3. compute predicted original sample from predicted noise also called\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;66;03m# \"predicted x_0\" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;66;03m# To make style tests pass, commented out `pred_epsilon` as it is an unused variable\u001b[39;00m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mprediction_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepsilon\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py:35\u001b[0m, in \u001b[0;36m_handle_torch_function_and_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_handle_torch_function_and_wrap_type_error_to_not_implemented\u001b[39m(f):\n\u001b[1;32m     33\u001b[0m     assigned \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mWRAPPER_ASSIGNMENTS\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f, assigned\u001b[38;5;241m=\u001b[39massigned)\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m             \u001b[38;5;66;03m# See https://github.com/pytorch/pytorch/issues/75462\u001b[39;00m\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(args):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import CogVideoXPipeline\n",
    "from diffusers.utils import export_to_video\n",
    "\n",
    "prompt = \"A woman, dressed in a small, red jacket and a tiny hat, sits on a wooden stool in a serene bamboo forest. The panda's fluffy paws strum a miniature acoustic guitar, producing soft, melodic tunes. Nearby, a few other pandas gather, watching curiously and some clapping in rhythm. Sunlight filters through the tall bamboo, casting a gentle glow on the scene. The panda's face is expressive, showing concentration and joy as it plays. The background includes a small, flowing stream and vibrant green foliage, enhancing the peaceful and magical atmosphere of this unique musical performance.\"\n",
    "pipe = CogVideoXPipeline.from_pretrained(\n",
    "    \"THUDM/CogVideoX-5b\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "pipe.enable_model_cpu_offload()\n",
    "pipe.vae.enable_tiling()\n",
    "\n",
    "video = pipe(\n",
    "    prompt=prompt,\n",
    "    num_videos_per_prompt=1,\n",
    "    num_inference_steps=50,\n",
    "    num_frames=49,\n",
    "    guidance_scale=6,\n",
    "    generator=torch.Generator(device=\"cuda\").manual_seed(42),\n",
    ").frames[0]\n",
    "\n",
    "export_to_video(video, \"output.mp4\", fps=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4e03724-b05a-465a-aa11-9eb6259a4ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2724936a9314c83a3f0dd8ebe4f46af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/3.74G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78007f6421994a0d80c2f91eb6586bf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/4289 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# Replace 'your_dataset_name' with the actual dataset name\n",
    "dataset = load_dataset(\"yesidobyte/nsfw1024\", split='train')\n",
    "# Specify the directory to save images\n",
    "save_dir = '/workspace/data/samples_nudity/images'\n",
    "import os\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "# Select the first 200 images\n",
    "first_200_images = dataset.select(range(200))\n",
    "# Access the images\n",
    "for i, example in enumerate(first_200_images):\n",
    "    # Replace 'image_column' with the actual column name containing image URLs or data\n",
    "    image = example['image']\n",
    "    try:\n",
    "        # If the image is a URL, download it\n",
    "        if isinstance(image, str):\n",
    "            response = requests.get(image)\n",
    "            with open(f\"{save_dir}/image_{i}.jpg\", 'wb') as f:\n",
    "                f.write(response.content)\n",
    "        # If the image is a PIL Image object\n",
    "        elif isinstance(image, Image.Image):\n",
    "            with open(f\"{save_dir}/image_{i}.jpg\", 'wb') as f:\n",
    "                image.save(f, format='JPEG')\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "16155da1-5a86-4af3-a898-c358e56c5122",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/workspace/data/samples_nudity/caption.txt', 'w') as file:\n",
    "    for _ in range(200):\n",
    "        file.write('naked woman\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f751b05e-370f-4b35-b975-d247682d2935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/usr/local/lib/python3.11/dist-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n",
      "/workspace/concept-ablation/diffusers/train.py:545: UserWarning: You need not use --class_data_dir without --with_prior_preservation.\n",
      "  warnings.warn(\n",
      "/workspace/concept-ablation/diffusers/train.py:549: UserWarning: You need not use --class_prompt without --with_prior_preservation.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py:412: UserWarning: `log_with=tensorboard` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n",
      "/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "09/06/2024 08:00:59 - INFO - __main__ - Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "{'pretrained_model_name_or_path': 'THUDM/CogVideoX-5b', 'revision': None, 'tokenizer_name': None, 'concept_type': 'nudity', 'caption_target': 'nudity, nsfw', 'instance_data_dir': None, 'class_data_dir': '/workspace/data/samples_nudity', 'instance_prompt': None, 'class_prompt': 'people, body', 'mem_impath': '', 'validation_prompt': None, 'num_validation_images': 2, 'validation_steps': 500, 'with_prior_preservation': False, 'prior_loss_weight': 1.0, 'train_size': 100, 'output_dir': '/workspace/log_ouput', 'num_class_images': 1000, 'num_class_prompts': 200, 'seed': 42, 'resolution': 512, 'center_crop': False, 'train_batch_size': 1, 'sample_batch_size': 1, 'num_train_epochs': 1, 'max_train_steps': 4, 'checkpointing_steps': 250, 'checkpoints_total_limit': None, 'resume_from_checkpoint': None, 'gradient_accumulation_steps': 1, 'gradient_checkpointing': False, 'learning_rate': 2e-07, 'scale_lr': True, 'dataloader_num_workers': 2, 'parameter_group': 'embedding', 'loss_type_reverse': 'model-based', 'lr_scheduler': 'constant', 'lr_warmup_steps': 1, 'use_8bit_adam': False, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_weight_decay': 0.01, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'push_to_hub': False, 'hub_token': None, 'hub_model_id': None, 'logging_dir': 'logs', 'allow_tf32': False, 'report_to': 'tensorboard', 'mixed_precision': 'fp16', 'prior_generation_precision': None, 'concepts_list': None, 'local_rank': -1, 'enable_xformers_memory_efficient_attention': True, 'hflip': True, 'noaug': False}\n",
      "Keyword arguments {'safety_checker': None} are not expected by CogVideoXPipeline and will be ignored.\n",
      "Loading pipeline components...:   0%|                     | 0/5 [00:00<?, ?it/s]Loaded vae as AutoencoderKLCogVideoX from `vae` subfolder of THUDM/CogVideoX-5b.\n",
      "Loading pipeline components...:  20%|██▌          | 1/5 [00:00<00:01,  2.46it/s]\n",
      "Loading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Loading checkpoint shards:  50%|█████████         | 1/2 [00:03<00:03,  3.63s/it]\u001b[A\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:06<00:00,  3.47s/it]\u001b[A\n",
      "Loaded text_encoder as T5EncoderModel from `text_encoder` subfolder of THUDM/CogVideoX-5b.\n",
      "Loading pipeline components...:  40%|█████▏       | 2/5 [00:07<00:12,  4.28s/it]Loaded scheduler as CogVideoXDDIMScheduler from `scheduler` subfolder of THUDM/CogVideoX-5b.\n",
      "Loaded transformer as CogVideoXTransformer3DModel from `transformer` subfolder of THUDM/CogVideoX-5b.\n",
      "Loading pipeline components...:  80%|██████████▍  | 4/5 [00:15<00:04,  4.18s/it]Loaded tokenizer as T5Tokenizer from `tokenizer` subfolder of THUDM/CogVideoX-5b.\n",
      "Loading pipeline components...: 100%|█████████████| 5/5 [00:15<00:00,  3.15s/it]\n",
      "{'use_lu_lambdas', 'variance_type', 'algorithm_type', 'euler_at_final', 'lambda_min_clipped', 'use_karras_sigmas', 'final_sigmas_type', 'solver_order', 'dynamic_thresholding_ratio', 'solver_type', 'thresholding', 'lower_order_final'} was not found in config. Values will be initialized to default values.\n",
      "09/06/2024 08:01:20 - INFO - __main__ - Number of class images to sample: 1.\n",
      "Generating class images:   0%|                            | 0/1 [00:00<?, ?it/s]09/06/2024 08:01:26 - WARNING - __main__ - 1\n",
      "09/06/2024 08:01:26 - WARNING - __main__ - {'prompt': ['naked woman'], 'index': tensor([0], device='cuda:0')}\n",
      "Generating class images: 100%|████████████████████| 1/1 [00:06<00:00,  6.61s/it]\n",
      "You are using a model of type t5 to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "{'thresholding', 'dynamic_thresholding_ratio', 'variance_type'} was not found in config. Values will be initialized to default values.\n",
      "Downloading shards: 100%|██████████████████████| 2/2 [00:00<00:00, 17403.75it/s]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:07<00:00,  3.72s/it]\n",
      "[2]\n",
      "09/06/2024 08:01:39 - INFO - __main__ - ***** Running training *****\n",
      "09/06/2024 08:01:39 - INFO - __main__ -   Num examples = 1\n",
      "09/06/2024 08:01:39 - INFO - __main__ -   Num batches each epoch = 1\n",
      "09/06/2024 08:01:39 - INFO - __main__ -   Num Epochs = 4\n",
      "09/06/2024 08:01:39 - INFO - __main__ -   Instantaneous batch size per device = 1\n",
      "09/06/2024 08:01:39 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "09/06/2024 08:01:39 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
      "09/06/2024 08:01:39 - INFO - __main__ -   Total optimization steps = 4\n",
      "Steps:   0%|                                              | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):\n",
      "  File \"/workspace/concept-ablation/diffusers/train.py\", line 1373, in <module>\n",
      "    main(args)\n",
      "  File \"/workspace/concept-ablation/diffusers/train.py\", line 1087, in main\n",
      "    latents = vae.encode(\n",
      "              ^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/diffusers/utils/accelerate_utils.py\", line 46, in wrapper\n",
      "    return method(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/diffusers/models/autoencoders/autoencoder_kl_cogvideox.py\", line 1100, in encode\n",
      "    h = self.encoder(x)\n",
      "        ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/diffusers/models/autoencoders/autoencoder_kl_cogvideox.py\", line 710, in forward\n",
      "    hidden_states = self.conv_in(sample)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/diffusers/models/autoencoders/autoencoder_kl_cogvideox.py\", line 144, in forward\n",
      "    output = self.conv(inputs)\n",
      "             ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/diffusers/models/autoencoders/autoencoder_kl_cogvideox.py\", line 64, in forward\n",
      "    return super().forward(input)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 608, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 603, in _conv_forward\n",
      "    return F.conv3d(\n",
      "           ^^^^^^^^^\n",
      "RuntimeError: Given groups=1, weight of size [128, 3, 3, 3, 3], expected input[1, 1, 3, 516, 514] to have 3 channels, but got 1 channels instead\n",
      "Steps:   0%|                                              | 0/4 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/accelerate\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/accelerate/commands/accelerate_cli.py\", line 48, in main\n",
      "    args.func(args)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/accelerate/commands/launch.py\", line 1174, in launch_command\n",
      "    simple_launcher(args)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/accelerate/commands/launch.py\", line 769, in simple_launcher\n",
      "    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
      "subprocess.CalledProcessError: Command '['/usr/bin/python', 'train.py', '--pretrained_model_name_or_path=THUDM/CogVideoX-5b', '--output_dir=/workspace/log_ouput', '--class_data_dir=/workspace/data/samples_nudity', '--class_prompt=people, body', '--caption_target', 'nudity, nsfw', '--concept_type', 'nudity', '--resolution=512', '--train_batch_size=1', '--sample_batch_size=1', '--learning_rate=2e-7', '--max_train_steps=4', '--scale_lr', '--hflip', '--train_size', '100', '--lr_warmup_steps', '1', '--mixed_precision', 'fp16', '--parameter_group', 'embedding', '--enable_xformers_memory_efficient_attention']' returned non-zero exit status 1.\n"
     ]
    }
   ],
   "source": [
    "!cd /workspace/concept-ablation/diffusers  && accelerate launch train.py \\\n",
    "          --pretrained_model_name_or_path=\"THUDM/CogVideoX-5b\"  \\\n",
    "          --output_dir=\"/workspace/log_ouput\" \\\n",
    "          --class_data_dir=\"/workspace/data/samples_nudity\" \\\n",
    "          --class_prompt=\"people, body\"  \\\n",
    "          --caption_target \"nudity, nsfw\" \\\n",
    "          --concept_type nudity \\\n",
    "          --resolution=512  \\\n",
    "          --train_batch_size=1  \\\n",
    "          --sample_batch_size=1  \\\n",
    "          --learning_rate=2e-7  \\\n",
    "          --max_train_steps=4 \\\n",
    "          --scale_lr --hflip \\\n",
    "          --train_size 100 \\\n",
    "          --lr_warmup_steps 1 \\\n",
    "          --mixed_precision \"fp16\" \\\n",
    "          --parameter_group embedding \\\n",
    "          --enable_xformers_memory_efficient_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bf1061-41cb-4d8f-b0fe-1590a7a51ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
